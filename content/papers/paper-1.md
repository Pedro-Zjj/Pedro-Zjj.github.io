---
title: 基于Transformer的代码生成模型研究
authors: Jiajun Zhang, Wei Liu, Ming Chen
venue: EMNLP 2024
year: 2024
rank: CCF-B
tags: [Code Generation, Transformer, Deep Learning]
pdf: https://example.com/paper.pdf
code: https://github.com/example/code
doi: 10.18653/v1/2024.emnlp-main.123
citations: 45
abstract: |
  本文提出了一种改进的代码生成模型，通过引入语法感知的注意力机制，显著提升了代码生成的准确性和可读性。
  在多个基准数据集上的实验结果表明，我们的方法相比现有最优方法提升了8.5%的准确率。
---

## 研究背景

代码生成是软件工程领域的重要研究方向，旨在通过自然语言描述自动生成可执行的程序代码。近年来，随着大语言模型的发展，代码生成技术取得了显著进步。

## 主要贡献

### 1. 语法感知的注意力机制
我们提出了一种新的注意力机制，能够更好地捕捉代码的语法结构信息。

### 2. 多任务学习框架
通过同时学习代码生成和代码摘要任务，提升了模型的泛化能力。

### 3. 大规模数据集构建
构建了一个包含100万条代码-注释对的数据集。

## 实验结果

在HumanEval和MBPP基准测试上，我们的方法取得了以下结果：

| 方法 | HumanEval@1 | MBPP@1 |
|------|-------------|--------|
| Codex | 28.8% | 45.9% |
| CodeGen | 29.3% | 49.2% |
| StarCoder | 33.6% | 52.7% |
| **我们的方法** | **42.1%** | **61.3%** |

## 结论

本文提出的语法感知代码生成模型在多个基准测试上取得了优异的性能，证明了语法信息对于代码生成任务的重要性。
